{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
=======
   "execution_count": 10,
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
<<<<<<< HEAD
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from keras import optimizers"
=======
    "import numpy as np\n",
    "from keras import optimizers\n"
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 35,
=======
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to upload dataset \n",
    "### input X: will be the values of the features for each song \n",
    "### output Y: will be the playlist that the feature is placed in "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 28,
=======
   "execution_count": 4,
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the data set: \n",
    "jazz = pd.read_csv(\"../data/jazz_csv.csv\")\n",
    "classical = pd.read_csv(\"../data/classical_csv.csv\")\n",
    "country = pd.read_csv(\"../data/country_csv.csv\")\n",
    "edm = pd.read_csv(\"../data/edm_csv.csv\")\n",
    "rap = pd.read_csv(\"../data/rap_csv.csv\")\n",
    "rock = pd.read_csv(\"../data/rock_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": 5,
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1vJOG1Ea4M7WtJlH2b9eM2</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.00975</td>\n",
       "      <td>7</td>\n",
       "      <td>-9.706</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>203.817</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4xR1tOO9p4cnDNmIEttfaZ</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.88600</td>\n",
       "      <td>3</td>\n",
       "      <td>-13.670</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>163.589</td>\n",
       "      <td>0.730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27NnHXU3w40RFJF9HEpazP</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.89300</td>\n",
       "      <td>8</td>\n",
       "      <td>-9.730</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>125.669</td>\n",
       "      <td>0.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75SK3djaJssHR9TTHTv7wI</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.58900</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.655</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>89.237</td>\n",
       "      <td>0.907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vwYvmB2B5h8GIfEVkwJj2</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.78300</td>\n",
       "      <td>5</td>\n",
       "      <td>-14.400</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>98.205</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Unnamed: 0  acousticness  danceability  energy  \\\n",
       "0  1vJOG1Ea4M7WtJlH2b9eM2         0.920         0.530   0.434   \n",
       "1  4xR1tOO9p4cnDNmIEttfaZ         0.995         0.376   0.467   \n",
       "2  27NnHXU3w40RFJF9HEpazP         0.990         0.495   0.476   \n",
       "3  75SK3djaJssHR9TTHTv7wI         0.826         0.688   0.404   \n",
       "4  5vwYvmB2B5h8GIfEVkwJj2         0.982         0.615   0.269   \n",
       "\n",
       "   instrumentalness  key  loudness  speechiness    tempo  valence  \n",
       "0           0.00975    7    -9.706       0.1430  203.817    0.829  \n",
       "1           0.88600    3   -13.670       0.0435  163.589    0.730  \n",
       "2           0.89300    8    -9.730       0.0413  125.669    0.900  \n",
       "3           0.58900    2   -10.655       0.0359   89.237    0.907  \n",
       "4           0.78300    5   -14.400       0.0508   98.205    0.752  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 29,
=======
     "execution_count": 5,
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jazz.head(5)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 41,
=======
   "execution_count": 6,
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   "metadata": {},
   "outputs": [],
   "source": [
    "#jazz_features, classical_features, country_features, country_features, edm_features, rap_features, rock_features = [], [],[],[],[],[]\n",
    "count = 0\n",
    "features = []\n",
    "genres = []\n",
    "#dataframes = {\"jazz\":jazz, \"classical\": classical, \"country\":country, \"edm\":edm, \"rap\": rap, \"rock\": rock}\n",
    "dataframes = {1:jazz, 2: classical, 3:country, 4:edm, 5: rap, 6: rock}\n",
    "for genre, dataframe in dataframes.items():\n",
    "    count = 0\n",
    "    for ind, row in dataframe.iterrows():\n",
<<<<<<< HEAD
    "        count+=1\n",
    "        features.append([row[\"acousticness\"], row[\"danceability\"], row[\"energy\"],row[\"loudness\"],row[\"speechiness\"], row[\"tempo\"], row[\"valence\"]])\n",
=======
    "        #print(row)\n",
    "        count+=1\n",
    "        features.append([row[\"acousticness\"], row[\"danceability\"], row[\"energy\"],  row[\"loudness\"], row[\"speechiness\"], row[\"tempo\"], row[\"valence\"]])\n",
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
    "        genres.append(genre)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 48,
=======
   "execution_count": 22,
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   "metadata": {},
   "outputs": [],
   "source": [
    "y = genres\n",
    "X = [features]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
=======
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "y = dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() #use a sequential model bc we wanna keep adding layers potentially \n",
    "model.add(Dense(256, input_dim=7, activation='relu'))\n",
    "#model.add(Dense(12, activation=\"relu\", input_shape=(7,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "#will prob need a dropout layer to compensate for overfitting"
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dummy_y"
=======
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#use mean squared error loss funciton - ideal for our model and adam optimizer \n",
    "#adam optimizer: might change later - Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n",
    "#https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "12548/12548 [==============================] - 2s 157us/step - loss: 0.3922 - accuracy: 0.8429\n",
      "Epoch 2/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.3180 - accuracy: 0.8598\n",
      "Epoch 3/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2873 - accuracy: 0.8701\n",
      "Epoch 4/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2728 - accuracy: 0.8745\n",
      "Epoch 5/150\n",
      "12548/12548 [==============================] - 2s 142us/step - loss: 0.2610 - accuracy: 0.8811\n",
      "Epoch 6/150\n",
      "12548/12548 [==============================] - 2s 152us/step - loss: 0.2530 - accuracy: 0.8862\n",
      "Epoch 7/150\n",
      "12548/12548 [==============================] - 2s 147us/step - loss: 0.2478 - accuracy: 0.8883\n",
      "Epoch 8/150\n",
      "12548/12548 [==============================] - 2s 145us/step - loss: 0.2455 - accuracy: 0.8912\n",
      "Epoch 9/150\n",
      "12548/12548 [==============================] - 2s 143us/step - loss: 0.2429 - accuracy: 0.8933\n",
      "Epoch 10/150\n",
      "12548/12548 [==============================] - 2s 145us/step - loss: 0.2414 - accuracy: 0.8931\n",
      "Epoch 11/150\n",
      "12548/12548 [==============================] - 2s 147us/step - loss: 0.2371 - accuracy: 0.8957\n",
      "Epoch 12/150\n",
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.2380 - accuracy: 0.8948\n",
      "Epoch 13/150\n",
      "12548/12548 [==============================] - 2s 148us/step - loss: 0.2355 - accuracy: 0.8961\n",
      "Epoch 14/150\n",
      "12548/12548 [==============================] - 2s 137us/step - loss: 0.2361 - accuracy: 0.8963\n",
      "Epoch 15/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2329 - accuracy: 0.8973\n",
      "Epoch 16/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2329 - accuracy: 0.8972\n",
      "Epoch 17/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2297 - accuracy: 0.8990\n",
      "Epoch 18/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2313 - accuracy: 0.8992\n",
      "Epoch 19/150\n",
      "12548/12548 [==============================] - 2s 152us/step - loss: 0.2291 - accuracy: 0.90050s - loss: 0.2292 - accuracy: \n",
      "Epoch 20/150\n",
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.2306 - accuracy: 0.89870s - loss:\n",
      "Epoch 21/150\n",
      "12548/12548 [==============================] - 2s 137us/step - loss: 0.2279 - accuracy: 0.9014\n",
      "Epoch 22/150\n",
      "12548/12548 [==============================] - 2s 138us/step - loss: 0.2275 - accuracy: 0.9006\n",
      "Epoch 23/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2275 - accuracy: 0.9003\n",
      "Epoch 24/150\n",
      "12548/12548 [==============================] - 2s 138us/step - loss: 0.2260 - accuracy: 0.9010\n",
      "Epoch 25/150\n",
      "12548/12548 [==============================] - 2s 146us/step - loss: 0.2249 - accuracy: 0.9022\n",
      "Epoch 26/150\n",
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.2260 - accuracy: 0.9020\n",
      "Epoch 27/150\n",
      "12548/12548 [==============================] - 2s 159us/step - loss: 0.2229 - accuracy: 0.9027\n",
      "Epoch 28/150\n",
      "12548/12548 [==============================] - 2s 137us/step - loss: 0.2223 - accuracy: 0.9037\n",
      "Epoch 29/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2229 - accuracy: 0.9034\n",
      "Epoch 30/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2228 - accuracy: 0.9036\n",
      "Epoch 31/150\n",
      "12548/12548 [==============================] - 2s 154us/step - loss: 0.2211 - accuracy: 0.9042\n",
      "Epoch 32/150\n",
      "12548/12548 [==============================] - 2s 167us/step - loss: 0.2217 - accuracy: 0.9036\n",
      "Epoch 33/150\n",
      "12548/12548 [==============================] - 2s 148us/step - loss: 0.2205 - accuracy: 0.9047\n",
      "Epoch 34/150\n",
      "12548/12548 [==============================] - 2s 138us/step - loss: 0.2184 - accuracy: 0.9054\n",
      "Epoch 35/150\n",
      "12548/12548 [==============================] - 2s 141us/step - loss: 0.2182 - accuracy: 0.9059\n",
      "Epoch 36/150\n",
      "12548/12548 [==============================] - 2s 157us/step - loss: 0.2189 - accuracy: 0.9050\n",
      "Epoch 37/150\n",
      "12548/12548 [==============================] - 2s 137us/step - loss: 0.2198 - accuracy: 0.9050\n",
      "Epoch 38/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2195 - accuracy: 0.9052\n",
      "Epoch 39/150\n",
      "12548/12548 [==============================] - 2s 149us/step - loss: 0.2166 - accuracy: 0.9063\n",
      "Epoch 40/150\n",
      "12548/12548 [==============================] - 2s 134us/step - loss: 0.2187 - accuracy: 0.9058\n",
      "Epoch 41/150\n",
      "12548/12548 [==============================] - 2s 127us/step - loss: 0.2173 - accuracy: 0.9061\n",
      "Epoch 42/150\n",
      "12548/12548 [==============================] - 2s 128us/step - loss: 0.2177 - accuracy: 0.9054\n",
      "Epoch 43/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2176 - accuracy: 0.9059\n",
      "Epoch 44/150\n",
      "12548/12548 [==============================] - 2s 147us/step - loss: 0.2164 - accuracy: 0.9070\n",
      "Epoch 45/150\n",
      "12548/12548 [==============================] - 2s 138us/step - loss: 0.2155 - accuracy: 0.9068\n",
      "Epoch 46/150\n",
      "12548/12548 [==============================] - 2s 134us/step - loss: 0.2149 - accuracy: 0.90750s - loss: 0.213\n",
      "Epoch 47/150\n",
      "12548/12548 [==============================] - 2s 133us/step - loss: 0.2157 - accuracy: 0.9071\n",
      "Epoch 48/150\n",
      "12548/12548 [==============================] - 2s 132us/step - loss: 0.2158 - accuracy: 0.9069\n",
      "Epoch 49/150\n",
      "12548/12548 [==============================] - 2s 150us/step - loss: 0.2148 - accuracy: 0.90790s - loss: 0.2158 - \n",
      "Epoch 50/150\n",
      "12548/12548 [==============================] - 2s 157us/step - loss: 0.2149 - accuracy: 0.9079\n",
      "Epoch 51/150\n",
      "12548/12548 [==============================] - 2s 141us/step - loss: 0.2138 - accuracy: 0.9069\n",
      "Epoch 52/150\n",
      "12548/12548 [==============================] - 2s 171us/step - loss: 0.2128 - accuracy: 0.9083\n",
      "Epoch 53/150\n",
      "12548/12548 [==============================] - 2s 188us/step - loss: 0.2128 - accuracy: 0.9079\n",
      "Epoch 54/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2125 - accuracy: 0.9081\n",
      "Epoch 55/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2117 - accuracy: 0.9092\n",
      "Epoch 56/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2133 - accuracy: 0.90800s - loss: 0.2131 - accuracy: \n",
      "Epoch 57/150\n",
      "12548/12548 [==============================] - 2s 160us/step - loss: 0.2121 - accuracy: 0.9094\n",
      "Epoch 58/150\n",
      "12548/12548 [==============================] - 2s 170us/step - loss: 0.2123 - accuracy: 0.90840s\n",
      "Epoch 59/150\n",
      "12548/12548 [==============================] - 2s 149us/step - loss: 0.2113 - accuracy: 0.9093\n",
      "Epoch 60/150\n",
      "12548/12548 [==============================] - 2s 133us/step - loss: 0.2125 - accuracy: 0.9080\n",
      "Epoch 61/150\n",
      "12548/12548 [==============================] - 2s 127us/step - loss: 0.2121 - accuracy: 0.9086\n",
      "Epoch 62/150\n",
      "12548/12548 [==============================] - 2s 127us/step - loss: 0.2114 - accuracy: 0.9092\n",
      "Epoch 63/150\n",
      "12548/12548 [==============================] - 2s 128us/step - loss: 0.2101 - accuracy: 0.9099\n",
      "Epoch 64/150\n",
      "12548/12548 [==============================] - 2s 129us/step - loss: 0.2105 - accuracy: 0.9093\n",
      "Epoch 65/150\n",
      "12548/12548 [==============================] - 2s 128us/step - loss: 0.2111 - accuracy: 0.9087\n",
      "Epoch 66/150\n",
      "12548/12548 [==============================] - 2s 127us/step - loss: 0.2100 - accuracy: 0.9092\n",
      "Epoch 67/150\n",
      "12548/12548 [==============================] - 2s 127us/step - loss: 0.2104 - accuracy: 0.9091\n",
      "Epoch 68/150\n",
      "12548/12548 [==============================] - 2s 127us/step - loss: 0.2096 - accuracy: 0.9098\n",
      "Epoch 69/150\n",
      "12548/12548 [==============================] - 2s 138us/step - loss: 0.2099 - accuracy: 0.91010s - loss:\n",
      "Epoch 70/150\n",
      "12548/12548 [==============================] - 2s 143us/step - loss: 0.2097 - accuracy: 0.9106\n",
      "Epoch 71/150\n",
      "12548/12548 [==============================] - 2s 157us/step - loss: 0.2094 - accuracy: 0.9105\n",
      "Epoch 72/150\n",
      "12548/12548 [==============================] - 2s 141us/step - loss: 0.2082 - accuracy: 0.91070s - loss: 0\n",
      "Epoch 73/150\n",
      "12548/12548 [==============================] - 2s 135us/step - loss: 0.2084 - accuracy: 0.9096\n",
      "Epoch 74/150\n",
      "12548/12548 [==============================] - 2s 133us/step - loss: 0.2082 - accuracy: 0.91060s - loss: 0.2083 - accuracy: 0.\n",
      "Epoch 75/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12548/12548 [==============================] - 2s 143us/step - loss: 0.2069 - accuracy: 0.9115\n",
      "Epoch 76/150\n",
      "12548/12548 [==============================] - 2s 141us/step - loss: 0.2078 - accuracy: 0.9109\n",
      "Epoch 77/150\n",
      "12548/12548 [==============================] - 2s 130us/step - loss: 0.2087 - accuracy: 0.9094\n",
      "Epoch 78/150\n",
      "12548/12548 [==============================] - 2s 161us/step - loss: 0.2071 - accuracy: 0.9116\n",
      "Epoch 79/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2058 - accuracy: 0.91150s -\n",
      "Epoch 80/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.2068 - accuracy: 0.9116\n",
      "Epoch 81/150\n",
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.2069 - accuracy: 0.9102\n",
      "Epoch 82/150\n",
      "12548/12548 [==============================] - 2s 155us/step - loss: 0.2064 - accuracy: 0.9114\n",
      "Epoch 83/150\n",
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.2059 - accuracy: 0.9114\n",
      "Epoch 84/150\n",
      "12548/12548 [==============================] - 2s 132us/step - loss: 0.2061 - accuracy: 0.91120s -\n",
      "Epoch 85/150\n",
      "12548/12548 [==============================] - 2s 133us/step - loss: 0.2062 - accuracy: 0.9116\n",
      "Epoch 86/150\n",
      "12548/12548 [==============================] - 2s 133us/step - loss: 0.2040 - accuracy: 0.9121\n",
      "Epoch 87/150\n",
      "12548/12548 [==============================] - 2s 134us/step - loss: 0.2052 - accuracy: 0.9121\n",
      "Epoch 88/150\n",
      "12548/12548 [==============================] - 2s 135us/step - loss: 0.2041 - accuracy: 0.9128\n",
      "Epoch 89/150\n",
      "12548/12548 [==============================] - 2s 160us/step - loss: 0.2053 - accuracy: 0.9120\n",
      "Epoch 90/150\n",
      "12548/12548 [==============================] - 2s 154us/step - loss: 0.2060 - accuracy: 0.9117\n",
      "Epoch 91/150\n",
      "12548/12548 [==============================] - 2s 163us/step - loss: 0.2043 - accuracy: 0.91230s - loss: 0.2044 - ac\n",
      "Epoch 92/150\n",
      "12548/12548 [==============================] - 2s 146us/step - loss: 0.2044 - accuracy: 0.9109\n",
      "Epoch 93/150\n",
      "12548/12548 [==============================] - 2s 143us/step - loss: 0.2050 - accuracy: 0.9123\n",
      "Epoch 94/150\n",
      "12548/12548 [==============================] - 2s 137us/step - loss: 0.2047 - accuracy: 0.9121\n",
      "Epoch 95/150\n",
      "12548/12548 [==============================] - 2s 148us/step - loss: 0.2031 - accuracy: 0.9126\n",
      "Epoch 96/150\n",
      "12548/12548 [==============================] - 2s 137us/step - loss: 0.2032 - accuracy: 0.9128\n",
      "Epoch 97/150\n",
      "12548/12548 [==============================] - 2s 135us/step - loss: 0.2044 - accuracy: 0.9124\n",
      "Epoch 98/150\n",
      "12548/12548 [==============================] - 2s 138us/step - loss: 0.2022 - accuracy: 0.9141\n",
      "Epoch 99/150\n",
      "12548/12548 [==============================] - 2s 158us/step - loss: 0.2019 - accuracy: 0.9136\n",
      "Epoch 100/150\n",
      "12548/12548 [==============================] - 2s 157us/step - loss: 0.2033 - accuracy: 0.9125\n",
      "Epoch 101/150\n",
      "12548/12548 [==============================] - 2s 164us/step - loss: 0.2029 - accuracy: 0.9131\n",
      "Epoch 102/150\n",
      "12548/12548 [==============================] - 2s 155us/step - loss: 0.2019 - accuracy: 0.9133\n",
      "Epoch 103/150\n",
      "12548/12548 [==============================] - 2s 146us/step - loss: 0.2038 - accuracy: 0.9125\n",
      "Epoch 104/150\n",
      "12548/12548 [==============================] - 2s 150us/step - loss: 0.2020 - accuracy: 0.9131\n",
      "Epoch 105/150\n",
      "12548/12548 [==============================] - 2s 156us/step - loss: 0.2015 - accuracy: 0.9132\n",
      "Epoch 106/150\n",
      "12548/12548 [==============================] - 2s 146us/step - loss: 0.2020 - accuracy: 0.9137\n",
      "Epoch 107/150\n",
      "12548/12548 [==============================] - 2s 152us/step - loss: 0.2012 - accuracy: 0.9136\n",
      "Epoch 108/150\n",
      "12548/12548 [==============================] - 2s 144us/step - loss: 0.2018 - accuracy: 0.9143\n",
      "Epoch 109/150\n",
      "12548/12548 [==============================] - 2s 148us/step - loss: 0.1999 - accuracy: 0.91490s - loss: 0.1995 - accuracy\n",
      "Epoch 110/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2016 - accuracy: 0.9131\n",
      "Epoch 111/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2029 - accuracy: 0.9120\n",
      "Epoch 112/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2009 - accuracy: 0.9141\n",
      "Epoch 113/150\n",
      "12548/12548 [==============================] - 2s 137us/step - loss: 0.2018 - accuracy: 0.9142\n",
      "Epoch 114/150\n",
      "12548/12548 [==============================] - 2s 149us/step - loss: 0.1997 - accuracy: 0.9143\n",
      "Epoch 115/150\n",
      "12548/12548 [==============================] - 2s 146us/step - loss: 0.2010 - accuracy: 0.9136\n",
      "Epoch 116/150\n",
      "12548/12548 [==============================] - 2s 157us/step - loss: 0.2010 - accuracy: 0.9146\n",
      "Epoch 117/150\n",
      "12548/12548 [==============================] - 2s 155us/step - loss: 0.2023 - accuracy: 0.9132\n",
      "Epoch 118/150\n",
      "12548/12548 [==============================] - 2s 151us/step - loss: 0.2007 - accuracy: 0.91411s - loss: 0.1996 - accuracy: \n",
      "Epoch 119/150\n",
      "12548/12548 [==============================] - 2s 154us/step - loss: 0.2000 - accuracy: 0.9147\n",
      "Epoch 120/150\n",
      "12548/12548 [==============================] - 2s 146us/step - loss: 0.2009 - accuracy: 0.9131\n",
      "Epoch 121/150\n",
      "12548/12548 [==============================] - 2s 152us/step - loss: 0.1999 - accuracy: 0.9138\n",
      "Epoch 122/150\n",
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.1995 - accuracy: 0.9143\n",
      "Epoch 123/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2013 - accuracy: 0.9147\n",
      "Epoch 124/150\n",
      "12548/12548 [==============================] - 2s 135us/step - loss: 0.1998 - accuracy: 0.9142\n",
      "Epoch 125/150\n",
      "12548/12548 [==============================] - 2s 139us/step - loss: 0.2002 - accuracy: 0.91460s -\n",
      "Epoch 126/150\n",
      "12548/12548 [==============================] - 2s 173us/step - loss: 0.1998 - accuracy: 0.91510s - loss: 0.1\n",
      "Epoch 127/150\n",
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.1999 - accuracy: 0.9146\n",
      "Epoch 128/150\n",
      "12548/12548 [==============================] - 2s 135us/step - loss: 0.1987 - accuracy: 0.9147\n",
      "Epoch 129/150\n",
      "12548/12548 [==============================] - 2s 138us/step - loss: 0.2014 - accuracy: 0.9134\n",
      "Epoch 130/150\n",
      "12548/12548 [==============================] - 2s 131us/step - loss: 0.1985 - accuracy: 0.9147\n",
      "Epoch 131/150\n",
      "12548/12548 [==============================] - 2s 134us/step - loss: 0.1981 - accuracy: 0.9157\n",
      "Epoch 132/150\n",
      "12548/12548 [==============================] - 2s 143us/step - loss: 0.1983 - accuracy: 0.9148\n",
      "Epoch 133/150\n",
      "12548/12548 [==============================] - 2s 142us/step - loss: 0.1991 - accuracy: 0.9152\n",
      "Epoch 134/150\n",
      "12548/12548 [==============================] - 2s 130us/step - loss: 0.1973 - accuracy: 0.9158\n",
      "Epoch 135/150\n",
      "12548/12548 [==============================] - 2s 155us/step - loss: 0.1981 - accuracy: 0.9156\n",
      "Epoch 136/150\n",
      "12548/12548 [==============================] - 2s 162us/step - loss: 0.1974 - accuracy: 0.9152\n",
      "Epoch 137/150\n",
      "12548/12548 [==============================] - 2s 147us/step - loss: 0.1976 - accuracy: 0.9155\n",
      "Epoch 138/150\n",
      "12548/12548 [==============================] - 2s 162us/step - loss: 0.1982 - accuracy: 0.91530s - loss: 0.1993 - accuracy\n",
      "Epoch 139/150\n",
      "12548/12548 [==============================] - 2s 182us/step - loss: 0.1988 - accuracy: 0.91470s - loss: 0\n",
      "Epoch 140/150\n",
      "12548/12548 [==============================] - 2s 145us/step - loss: 0.1987 - accuracy: 0.91440s - l\n",
      "Epoch 141/150\n",
      "12548/12548 [==============================] - 2s 150us/step - loss: 0.1985 - accuracy: 0.9158\n",
      "Epoch 142/150\n",
      "12548/12548 [==============================] - 2s 153us/step - loss: 0.1974 - accuracy: 0.9152\n",
      "Epoch 143/150\n",
      "12548/12548 [==============================] - 2s 145us/step - loss: 0.1982 - accuracy: 0.9149\n",
      "Epoch 144/150\n",
      "12548/12548 [==============================] - 2s 136us/step - loss: 0.1976 - accuracy: 0.9161\n",
      "Epoch 145/150\n",
      "12548/12548 [==============================] - 2s 135us/step - loss: 0.1982 - accuracy: 0.9155\n",
      "Epoch 146/150\n",
      "12548/12548 [==============================] - 2s 133us/step - loss: 0.1984 - accuracy: 0.9158\n",
      "Epoch 147/150\n",
      "12548/12548 [==============================] - 2s 134us/step - loss: 0.1971 - accuracy: 0.9156\n",
      "Epoch 148/150\n",
      "12548/12548 [==============================] - 2s 160us/step - loss: 0.1975 - accuracy: 0.9151\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12548/12548 [==============================] - 2s 140us/step - loss: 0.1963 - accuracy: 0.9151\n",
      "Epoch 150/150\n",
      "12548/12548 [==============================] - 2s 152us/step - loss: 0.1965 - accuracy: 0.9163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1418dec90>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
   "source": [
    "model = Sequential() #use a sequential model bc we wanna keep adding layers potentially \n",
    "model.add(Dense(256, input_dim=7, activation='relu'))\n",
    "#model.add(Dense(12, activation=\"relu\", input_shape=(7,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "#will prob need a dropout layer to compensate for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#use mean squared error loss funciton - ideal for our model and adam optimizer \n",
    "#adam optimizer: might change later - Adam is an optimization algorithm that can used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12548/12548 [==============================] - 1s 65us/step - loss: 0.2032 - acc: 0.9132\n",
      "Epoch 2/10\n",
      "12548/12548 [==============================] - 1s 65us/step - loss: 0.2332 - acc: 0.8971\n",
      "Epoch 3/10\n",
      "12548/12548 [==============================] - 1s 59us/step - loss: 0.2141 - acc: 0.9081\n",
      "Epoch 4/10\n",
      "12548/12548 [==============================] - 1s 61us/step - loss: 0.2140 - acc: 0.9078\n",
      "Epoch 5/10\n",
      "12548/12548 [==============================] - 1s 64us/step - loss: 0.2098 - acc: 0.9102\n",
      "Epoch 6/10\n",
      "12548/12548 [==============================] - 1s 60us/step - loss: 0.2079 - acc: 0.9108\n",
      "Epoch 7/10\n",
      "12548/12548 [==============================] - 1s 57us/step - loss: 0.2056 - acc: 0.9122\n",
      "Epoch 8/10\n",
      "12548/12548 [==============================] - 1s 58us/step - loss: 0.2054 - acc: 0.9117\n",
      "Epoch 9/10\n",
      "12548/12548 [==============================] - 1s 64us/step - loss: 0.2036 - acc: 0.9129\n",
      "Epoch 10/10\n",
      "12548/12548 [==============================] - 1s 56us/step - loss: 0.2029 - acc: 0.9136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30f68780>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=10, batch_size=128) #start with a baby amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12548/12548 [==============================] - 1s 69us/step\n",
      "Accuracy: 91.36\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X, y) #this is the loss and the accuracy \n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< HEAD
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
=======
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
>>>>>>> 8e1ae9716d96d0ee8e5a53ff2197feef8ce08988
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
